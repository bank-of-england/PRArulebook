---
title: "Introduction to PRArulebook package"
author: "Eryk Walczak"
date: "`r Sys.Date()`" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction-to-PRA-rulebook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(PRArulebook)
```

Get the structure:

```{r}
sectors <- scrape_sector_structure("http://www.prarulebook.co.uk/rulebook/Home/Handbook/22-03-2006")
parts <- scrape_part_structure(sectors)
```

See what was scraped:

```{r, warning = FALSE, message = FALSE}
library(dplyr)

dplyr::glimpse(parts)
```


Visualise the structure:

```{r, warning = FALSE, message = FALSE}
library(collapsibleTree)

Handbook <- parts

collapsibleTree(
  Handbook,
  hierarchy = c("sector_name", "part_name"),
  width = 900,
  height = 1100,
  zoomable = FALSE,
  collapsed = FALSE
)
```

Obtaining rule-level content (including rule URLs) takes a bit longer. First, chapters need to be scraped. Then data frame containing chapters can be used to obtain rules:

```{r, eval = FALSE}
chapters <- scrape_chapter_structure(parts)
```

```{r, eval = FALSE}
rules <-
  scrape_rule_structure(chapters,
                        date = "22-03-2006")
```


```{r}
#test
for (i in 1:nrow(chapters)) {
  print(i)
  #print(chapters[i,])
  scrape_rule_structure(chapters[i,],
                        date = "22-03-2006")
}
```



This will generate a data frame with rule-level structure. The next step (if you require the lowest level of the data) is obtaining the rule IDs and text. This is *very slow* as individual rule IDs are not so easy to extract and the scraper needs to visit every single rule URL.

```{r, eval = FALSE}
rules_df_id <- list()

for (i in 1:nrow(rules_df)) {
  print(i)
  rules_df_id[[i]] <-
    scrape_rule_id(rules_df$rule_url[i],
                   rules_df$rule_number_sel[i],
                   rules_df$rule_text_sel[i])
}

rules_df_id_df <- dplyr::bind_rows(rules_df_id)
```

## Faster scraping

*future* and *furrr* packages were tested to speed up the process of scraping, but this method often resulted in errors so eventually it was not used in this package. Instead, *purrr* was used.

Here is an example of using *furrr* to acquire chapter-level data:

```{r, eval = FALSE}
# scrape part-level data
df <-
  get_structure("01-01-2010",
                layer = "part")

# start multicore processing
library(future)
plan(multiprocess)

# get all chapters and append to a data frame
chapters <-
  furrr::future_map_dfr(df$part_url,
                scrape_menu, selector = ".Chapter a",
                .progress = TRUE)
```

