---
title: "Introduction to PRArulebook package"
author: "Eryk Walczak"
date: "`r Sys.Date()`" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction-to-PRA-rulebook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, eval = FALSE}
library(PRArulebook)
```

Get the structure:

```{r, eval = FALSE}
sectors <- scrape_sector_structure("http://www.prarulebook.co.uk/rulebook/Home/Handbook/22-03-2006")
parts <- scrape_part_structure(sectors)
```

See what was scraped:

```{r, warning = FALSE, message = FALSE, eval = FALSE}
library(dplyr)

dplyr::glimpse(parts)
```


Visualise the structure:

```{r, warning = FALSE, message = FALSE, eval = FALSE}
library(collapsibleTree)

Handbook <- parts

collapsibleTree(
  Handbook,
  hierarchy = c("sector_name", "part_name"),
  width = 900,
  height = 1100,
  zoomable = FALSE,
  collapsed = FALSE
)
```

Obtaining rule-level content (including rule URLs) takes a bit longer. First, chapters need to be scraped. Then data frame containing chapters can be used to obtain rules:

```{r, eval = FALSE}
chapters <- scrape_chapter_structure(parts)
```

```{r, eval = FALSE}
rules <-
  scrape_rule_structure(chapters[1:3],
                        rulebook_date = "22-03-2006")
```

This will generate a data frame with rule-level structure. The next step (if you require the lowest level of the data) is obtaining the rule IDs and text. This is *very slow* as individual rule IDs are not so easy to extract and the scraper needs to visit every single rule URL.

```{r, eval = FALSE}
# `get_content` needs to be called on each rule that needs to be scraped
rule_text <- get_content(rules$rule_url[1], "text", "yes")
```

## Faster scraping

*future* and *furrr* packages were tested to speed up the process of scraping, but this method often resulted in errors so eventually it was not used in this package. Instead, *purrr* was used.

Here is an example of using *furrr* to acquire chapter-level data:

```{r, eval = FALSE}
# scrape part-level data
df <-
  get_structure("01-01-2010",
                layer = "part")

# start multicore processing
library(future)
plan(multiprocess)

# get all chapters and append to a data frame
chapters <-
  furrr::future_map_dfr(df$part_url,
                scrape_menu, selector = ".Chapter a",
                .progress = TRUE)
```
